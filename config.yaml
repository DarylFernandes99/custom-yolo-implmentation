# ============================================================
#  GLOBAL PROJECT CONFIGURATION
# ============================================================

project:
  name: "multi_class_object_detection"
  description: "Multi-class object detection using Dask → Parquet → FSDP/DDP/FSDP2"
  seed: 42
  num_classes: 172
  device: "cuda"
  distributed: true
  mixed_precision: true
  output_dir: "experiments"
  log_dir: "./dataset/experiments/run_logs"
  profile_dir: "./dataset/experiments/profiles"

# ============================================================
#  DATA CONFIGURATION
# ============================================================

data:
  root_dir: "./dataset"
  raw_dir: "./dataset/raw"
  processed_dir: "./dataset/processed/parquet"
  metadata_dir: "./dataset/processed/metadata"
  annotations_dir: "./dataset/raw/annotations"

  train_parquet: "train"
  val_parquet: "val"

  train_images: "./dataset/raw/images/train"
  val_images: "./dataset/raw/images/val"
  test_images: "./dataset/raw/images/test"

  num_workers: 8
  pin_memory: true
  prefetch_factor: 2

  is_test: true

# ============================================================
#  MODEL CONFIGURATION
# ============================================================

model:
  input_size: [640, 640]
  num_classes: 172

  # config: {'csp': [False, True], 'depth' : [1, 1, 1, 1, 1, 1], 'width' : [3, 16, 32, 64, 128, 256]}
  # config: {'csp': [False, True], 'depth' : [1, 1, 1, 1, 1, 1], 'width' : [3, 32, 64, 128, 256, 512]}
  # config: {'csp': [True, True], 'depth' : [1, 1, 1, 1, 1, 1], 'width' : [3, 64, 128, 256, 512, 512]}
  # config: {'csp': [True, True], 'depth' : [2, 2, 2, 2, 2, 2], 'width' : [3, 64, 128, 256, 512, 512]}
  config: {'csp': [True, True], 'depth' : [2, 2, 2, 2, 2, 2], 'width' : [3, 96, 192, 384, 768, 768]}

# ============================================================
#  TRAINING CONFIGURATION
# ============================================================

training:
  is_test: true
  batch_size: 4
  epochs: 1
  learning_rate: 0.0001
  weight_decay: 0.0001
  optimizer: "adamw"
  scheduler: "reduce_on_plateau"
  grad_clip: 1.0
  early_stopping_patience: 5
  learning_rate_patience: 3
  learning_rate_factor: 0.5

  # FSDP / Distributed
  fsdp:
    sharding_strategy: "FULL_SHARD"
    auto_wrap_policy_min_params: 10000000
    precision: "bfloat16"   # One of ("bfloat16", "float16", "float32")
  
  fsdp2:
    precision: "bfloat16"   # One of ("bfloat16", "float16", "float32")

  ddp:
    find_unused_parameters: false
    precision: "float32"    # One of ("bfloat16", "float16", "float32")

  # Training weights
  weights:
    cls_loss: 1.0
    bbox_loss: 1.5
    mask_loss: 0.5   # optional (if segmentation enabled)

# ============================================================
#  WANDB CONFIGURATION
# ============================================================

wandb:
  enable: true
  project_name: "hpc_project"
  entity: "daryl99"         # <-- replace with your wandb username/team
  run_name: "training_run"
  log_frequency: 1                     # log every N steps
  mode: "online"                        # "online" | "offline" | "disabled"

# ============================================================
#  CHECKPOINT & LOGGING CONFIGURATION
# ============================================================

checkpoint:
  save_interval: 1                      # save every N epochs
  resume_training: false
  best_model_metric: "val/loss"
  best_model_mode: "min"
  checkpoint_dir: "./dataset/experiments/checkpoints"

logging:
  console_log: true
  file_log: true
  log_level: "INFO"
